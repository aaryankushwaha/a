Practical 1A

Code :

import networkx as nx
import matplotlib.pyplot as plt

g = nx.Graph()
g.add_edges_from([(1,2),(2,8),(2,5),(1,3),(3,6),(3,9),(3,7),(1,4)])
nx.draw_spring(g,with_labels=True)
print("The BFS is : ",list(nx.bfs_tree(g,1)))
plt.show()

--------------------------------------------------------------------
Practical 1B

import networkx as nx
import matplotlib.pyplot as plt

g = nx.Graph()
g.add_edges_from([(1,2),(2,8),(2,5),(1,3),(3,6),(3,9),(3,7),(1,4)])
nx.draw_spring(g,with_labels=True)
print("The DFS is : ",list(nx.dfs_tree(g,1)))
plt.show()

---------------------------------------------------------------------------------------------------------
Practical 2

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

#Generate our dataset
dataset = make_blobs(n_samples=200, centers=4,
                                    n_features=2, cluster_std=1.6)
points = dataset[0]
kmeans = KMeans(n_clusters=4)
kmeans.fit(points) #fit the kmeans object to datasets
clusters = kmeans.cluster_centers_
print(clusters)

y_km = kmeans.fit_predict(points)
plt.scatter(points[y_km == 0,0], points[y_km == 0,1], s=50, color='red')
plt.scatter(points[y_km == 1,0], points[y_km == 1,1], s=50, color='blue')
plt.scatter(points[y_km == 2,0], points[y_km == 2,1], s=50, color='yellow')
plt.scatter(points[y_km == 3,0], points[y_km == 3,1], s=50, color='lightgreen')

plt.scatter(clusters[0,0], clusters[0,1], marker="*", s=200, color="black")
plt.scatter(clusters[1,0], clusters[1,1], marker="*", s=200, color="black")
plt.scatter(clusters[2,0], clusters[2,1], marker="*", s=200, color="black")
plt.scatter(clusters[3,0], clusters[3,1], marker="*", s=200, color="black")
plt.show()

---------------------------------------------------------------------------------------------------------
Practical 3

import numpy as np

def find_neighbours(state, landscape):
    neighbours = []
    dim = landscape.shape

    # left neighbour
    if state[0] != 0:
        neighbours.append((state[0] - 1, state[1]))

    # right neighbour
    if state[0] != dim[0] - 1:
        neighbours.append((state[0] + 1, state[1]))

    # top neighbour
    if state[1] != 0:
        neighbours.append((state[0], state[1] - 1))

    # bottom neighbour
    if state[1] != dim[1] - 1:
        neighbours.append((state[0], state[1] + 1))

    # top left
    if state[0] != 0 and state[1] != 0:
        neighbours.append((state[0] - 1, state[1] - 1))

    # bottom left
    if state[0] != 0 and state[1] != dim[1] - 1:
        neighbours.append((state[0] - 1, state[1] + 1))

    # top right
    if state[0] != dim[0] - 1 and state[1] != 0:
        neighbours.append((state[0] + 1, state[1] - 1))

    # bottom right
    if state[0] != dim[0] - 1 and state[1] != dim[1] - 1:
        neighbours.append((state[0] + 1, state[1] + 1))

    return neighbours


# Current optimization objective: local/global maximum
def hill_climb(curr_state, landscape):
    neighbours = find_neighbours(curr_state, landscape)
    bool
    ascended = False
    next_state = curr_state
    for neighbour in neighbours: #Find the neighbour with the greatest value
        if landscape[neighbour[0]][neighbour[1]] > landscape[next_state[0]][next_state[1]]:
            next_state = neighbour
            ascended = True

    return ascended, next_state


def __main__():
    landscape = np.random.randint(1, high=50, size=(10, 10))
    print(landscape)
    start_state = (3, 6)  # matrix index coordinates
    current_state = start_state
    count = 1
    ascending = True
    while ascending:
        print("\nStep #", count)
        print("Current state coordinates: ", current_state)
        print("Current state value: ", landscape[current_state[0]][current_state[1]])
        count += 1
        ascending, current_state = hill_climb(current_state, landscape)

    print("\nStep #", count)
    print("Optimization objective reached.")
    print("Final state coordinates: ", current_state)
    print("Final state value: ", landscape[current_state[0]][current_state[1]])


__main__()



----------------------------------------------------------------------------------------------------------------------------


Practical 4

import numpy as np
import pandas as pd
import csv
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.models import BayesianNetwork
from pgmpy.inference import VariableElimination

heartDisease = pd.read_csv('heart.csv')

print('Sample instances from the dataset are given below')
print(heartDisease.head())

print("\n Attributes and Datatypes")
print(heartDisease.dtypes)

model = BayesianNetwork([('age','heartdisease'),('sex','heartdisease'),('exang','heartdisease'),('cp','heartdisease'),('heartdisease','restecg'),('heartdisease','chol')])

print('\n Learning CPD using Maximum likelihood estimators')
model.fit(heartDisease,estimator=MaximumLikelihoodEstimator)

print('\n Inferencing with Bayesian Network:')
HeartDiseasetest_infer = VariableElimination(model)

print('\n 1.Probability of HeartDisease given evidence=restecg :1')
q1=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'restecg':1})
print(q1)


print('\n 2.Probability of HeartDisease given evidence= cp:2 ')
q2=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'cp':2})
print(q2)

----------------------------------------------------------------------------------------
Practical 5

import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
%matplotlib inline
def load_dataset():
    train_dataset = h5py.File('/content/drive/MyDrive/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('/content/drive/MyDrive/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y_orig, classes = load_dataset()
index = 10
plt.imshow(train_set_x_orig[index])
print ("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") +  "' picture.")
#index= 26
#plt.imshow(test_set_x_orig[index])
#print ("y = " + str(test_set_y[:, index]) + ", it's a '" + classes[np.squeeze(test_set_y[:, index])].decode("utf-8") +  "' picture.")


------------------------------------------------------------------------------------------------------------------------------------------------

Practical 6

import numpy as np
def unitStep(v):
  if v>=0:
    return 1
  else:
    return 0
 
def perceptronModel(x,w,b):
  v=np.dot(w,x)+b
  y=unitStep(v)
  return y
 
def OR_logicfunction(x):
  w=np.array([1,1])
  b=-0.5
  return perceptronModel(x,w,b)
 
def AND_logicFunction(x): 
 w = np.array([1, 1]) 
 bAND = -1.5
 return perceptronModel(x, w, bAND) 
 
 
 
test1=np.array([0,0])
test2=np.array([0,1])
test3=np.array([1,0])
test4=np.array([1,1])
print("OR({},{})={}".format(0,0,OR_logicfunction(test1)))
print("OR({},{})={}".format(0,1,OR_logicfunction(test2)))
print("OR({},{})={}".format(1,0,OR_logicfunction(test3)))
print("OR({},{})={}".format(1,1,OR_logicfunction(test4)))
 
print("AND({},{})={}".format(0,1,AND_logicFunction(test1)))
print("AND({},{})={}".format(1,1,AND_logicFunction(test2)))
print("AND({},{})={}".format(0,0,AND_logicFunction(test3)))
print("AND({},{})={}".format(1,0,AND_logicFunction(test4)))

------------------------------------------------------------------------------------------------------------------------------------
Practical 8

import numpy as np
import pandas as pd
import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout
import matplotlib.pyplot as mp
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
print(train_images.shape, train_labels.shape)
print(test_images.shape , test_labels.shape)
class_names = ["zero","one","two","three","four","five","six","seven","eight","nine"]
print(train_images[0,5:22,5:22])
mp.figure()
mp.imshow(train_images[0])
mp.colorbar()
train_images = train_images/255.0
test_images = test_images/255.0
mp.figure(figsize=(10,10))
for i in range(20):
 mp.subplot(4,5, i+1)
 mp.xticks([])
 mp.yticks([])
 mp.grid(False)
 mp.imshow(train_images[i])
 mp.xlabel(class_names[train_labels[i]])
model = Sequential()
model.add(Conv2D(32, (3,3), activation='relu', input_shape= (28,28,1)))
model.add(MaxPool2D((2,2)))
model.add(Conv2D(48, (3,3), activation='relu'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.summary()
model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy',metrics= ['accuracy'] )
model.fit(train_images, train_labels, epochs=10, batch_size = 128, verbose= 2, validation_data=(train_images, train_labels) )
loss, accuracy= model.evaluate(test_images, test_labels, verbose = 1)
loss * 100
print(f'Accuracy: {accuracy*100}')
mp.imshow(train_images[2])
prediction = model.predict(train_images)
print(np.argmax(prediction[2]))


